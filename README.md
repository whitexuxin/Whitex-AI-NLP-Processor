
# Whitex-AI-NLP-Processor\nThis project, originally developed by User 18F, is now maintained by whitexuxin. The AI-NLP-Processor (formerly known as 10x-MeL) features MeL, a machine learning and natural language processing tool for analysing open text data.\n\n## Setup and Usage\n\n### How to install\n\n1. Install **Docker Desktop**\n   1. Go to https://www.docker.com/products/docker-desktop\n   2. Install the **Docker Desktop** for your platform\n   3. Make sure Docker is running\n2. Install **git**\n   1. Go to https://git-scm.com/download/\n   2. Install the **git** for your platform\n3. Create a folder on your machine for the **MeL** tool\n   1. Choose a location for the **MeL** tool on your machine, for example, in your **Documents** folder.\n   2. Open a **Terminal** (Mac/Linux) or **Command Prompt** (Windows) and navigate to the folder you chose in the previous step:\n      -  `cd path/to/that/folder`\n      - for example, on a Mac: `cd ~/Documents`\n   3. Create a new folder for the **MeL** tool, for example `mel`:\n      - `mkdir mel`\n   4. Move to the newly created folder:\n      -  `cd mel`\n4. Obtain the **Mel** tool from **GitHub**\n   1. Type the following to copy the **MeL** tool to your machine:\n      - `git clone https://github.com/whitexuxin/Whitex-AI-NLP-Processor.git`\n   2. Navigate into the tool by typing:\n      - `cd Whitex-AI-NLP-Processor`\n5. Connect the **MeL** tool to Docker by typing:\n   - `docker-compose up --build -d`\n\n### Using the tool\n\n1. Open **Docker Desktop**\n2. Observe the listing for `Whitex-AI-NLP-Processor` and the presence of a **play button**\n3. Click the **play button**\n4. Once the status is `RUNNING` or the tool icon turns **green**, go to\n   - http://localhost:5000/\n   - This page can also be opened but double-clicking `start.webloc` in the `Mel` folder\n\n## Tools\n\n### AutoCat\n\n#### Purpose\n\nTo discover latent categories in the text and assign entries to those categories.\n\n#### Approach\n\nThere are several components to automatic categorization:\n\n##### Category Discovery\n\n		1. The text in each entry is parsed and noun phrases are extracted\n  		2. Count occurrences of words and phrases contained in the noun phrases\n       - Counts are boosted according to the the recency of the entries in which they appear\n  		3. Use the most commonly occurring words and phrases as the initial category headings\n  		4. Perform an initial pass over the categories, merging those categories that share common terms\n  		5. Create language models for each category\n  		6. Perform another pass over the categories, this time merging categories whose language models are similar, using entropy as a metric\n\n##### Entry Prediction\n\n  		1. Entries are compared to the terms in the most popular categories, according to a power-law "rich get richer" approach. When terms intersect, the entry is linked to one or more category/subcategory pairs.\n  		2. If an entry fails to match any categories, a language model is created for the entry and it is compared with the language model of each category to determine best fit.\n\n### Problem Report\n\n#### Purpose\n\nTo detect user reports of problem they encounter with usa.gov and other government websites\n\n#### Approach\n\n##### Data\n\nTwo categories of data are used to detect problems:\n\n1. The text of users' survey responses\n2. The ratings provided by users, often on a scale from 1 to 5\n   - These ratings are normalized from a scale of 1 to 5 to a scale from -2 to +2, where complaints are negative.\n\n##### Evaluation\n\nThe task is distilled into two subtasks:\n\n###### Determining whether the entry provides a negative context\n\n- example: a complaint\n\nLinguistic analysis is combined with the users' ratings to determine the sentiment of the entry and the likelihood that the user is providing context about a problem that was experienced\n\n###### Determining relevance\n\n- example: a government website provides out-of-date information\n\nSurface pattern matching is used here to determine whether the content of the entry is relevant. This matching is list based, so it can be easily adapted for other use cases.\n\n## Contributing\n\nSee [CONTRIBUTING](CONTRIBUTING.md) for additional information.\n\n## Public domain\n\nThis project is in the worldwide [public domain](LICENSE.md). As stated in [CONTRIBUTING](CONTRIBUTING.md):\n\n> This project is in the public domain within the United States, and copyright and related rights in the work worldwide are waived through the [CC0 1.0 Universal public domain dedication](https://creativecommons.org/publicdomain/zero/1.0/).\n>\n> All contributions to this project will be released under the CC0 dedication. By submitting a pull request, you are agreeing to comply with this waiver of copyright interest.